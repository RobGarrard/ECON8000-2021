\documentclass[12pt]{article}

\usepackage{amsmath, amsfonts, amsthm, amssymb}
\usepackage[left = 1in, right = 1in]{geometry}
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{multicol}
\usepackage{verbatim}
\usepackage{booktabs}

% Include Solutions
\newif\ifsln
\slnfalse
\slntrue

% No Indent
\setlength\parindent{0pt}


% Common Sets
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\renewcommand{\epsilon}{\varepsilon}

\newcommand{\E}{\mathbb{E}}

\renewcommand{\iff}{\Leftrightarrow}
\newcommand{\halmos}{\hfill$\blacksquare$}

\newenvironment{amatrix}[1]{%
  \left(\begin{array}{@{}*{#1}{c}|c@{}}
}{%
  \end{array}\right)
}

\begin{document}
\pagestyle{fancyplain}


\chead{\textbf{Tutorial 3 \ifsln Solutions \fi}}
\lhead{\textbf{ECON8000}}
\rhead{\textbf{Semester 1, 2021}}

\begin{center}
Solutions due by 10.30am Friday 26\textsuperscript{th} February.
\end{center}

\begin{enumerate}[1.]
\setlength\itemsep{5mm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item Show that if a square matrix is lower triangular or upper trianguar, then its determinant is the product of its diagonal entries. 

Recall lower triangular matrices have zeros below the main diagonal: $\begin{pmatrix}1 &2 & 3 \\ 0 & 4 & 5\\ 0 & 0& 6\end{pmatrix}$.

\ifsln
\textit{Solution:}\\

Prove by induction on the size of the matrix. For the base case, $n = 1$, the determinant is the product of the diagonal since there's only one entry. For the induction step, assume the proposition is true for any $(n-1)\times (n-1)$ matrix and try to show it's true for an $n\times n$ matrix. \smallskip

Let $A$ be an $n\times n$ lower triangular matrix. Expanding along the first column, $\det(A) = a_{11}M_{11}$, since $a_{j1} = 0$ for $j \geq 1$. But $M_{11}$ is the determinant of an $(n-1)\times(n-1)$ matrix. By the induction hypothesis its determinant is the product of its main diagonal which is, $M_{11} = a_{22}a_{33}...a_{nn}$. Therefore $\det(A) = a_{11}a_{22}\dots a_{nn}$.Proof for upper triangular follows by transposition. \halmos
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item Suppose $A$ is diagonalizable. Show that $A^{n} = \underbrace{A\times A \times \dots A}_{n}$ can be expressed more simply as $A^{n} = P^{-1} D^{n} P$.

\ifsln
\textit{Solution:}\\
Since it's diagonalizable, it can be represented by $A = P^{-1}DP$, where $D$ is diagonal. Note that for a diagonal matrix, $\underbrace{D\times D \times \dots D}_{n} = D^{n}$ is just the matrix where $(D^{n})_{ii} = (D_{ii})^{n}$.\smallskip

$A^{n} = \underbrace{A\times A \times \dots A}_{n} = \underbrace{(P^{-1}DP)\times (P^{-1}DP)\times \dots (P^{-1}DP)}_{n}  = P^{-1}D(PP^{-1})DP\dots = P^{-1}D^{n}P$ since $PP^{-1} = I$.
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item Suppose $A$ is diagonalizable. Show that 
	\begin{enumerate}[a)]
		\item $trace(A) = \lambda_{1} + \dots + \lambda_{n}$.
		\item $\det(A) = \lambda_{1}\cdot \dots \cdot \lambda_{n}$.
	\end{enumerate}
Note that this is true in general for square matrices, whether they're diagonalizable or not, but the proof is harder. [Hint: you can use $\det(AB) = \det(A)\det(B)$.]

\ifsln
\textit{Solution:}\\
a) $trace(A) = trace(P^{-1}DP) = trace(P P^{-1}D)$ (by cyclic property) $ = trace(ID) = trace(D) = \sum_{i} \lambda_{i}$\\

b) $\det(A) = \det(P^{-1}DP) = \det(P^{-1}) \det(D) \det(P) = \det(D)$ (since $\det(P^{-1}) = 1 / \det(P)$) $ = \Pi_{i} \lambda_{i}$ (since diagonal matrices are lower triangular).
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item Consider an economy where agents can be in one of the states `employed' or `unemployed'. Suppose the probability that an employed person stays employed from one period to the next is 0.95. Suppose the probability that an unemployed person becomes employed in the next period is 0.7. 
\begin{enumerate}[a)]
	\item Write this setup as a system of first order difference equations.
	\item Solve the system in terms of eigenvalues and eigenvectors.
	\item What is the long run rate of unemployment is this economy?
\end{enumerate}

\ifsln
\textit{Solution:}\\
a) $x_{t+1} = A x_{t}$, where $x_{t} = \begin{bmatrix}e_{t}\\u_{t}\end{bmatrix}$ and $A = \begin{pmatrix}0.95 & 0.7\\ 0.05 & 0.3 \end{pmatrix}$.\\

b) $trace(A) = 1.25 = \lambda_{1} + \lambda_{2}$. $\det(A) = (0.95\cdot 0.3 - 0.7\cdot 0.05) = .25 = \lambda_{1}\lambda_{2}$. So $\lambda_{1}= 1$, $\lambda_{2} = 1/4$.\\

$\begin{amatrix}{2}1-0.95 & -0.7 & 0\\ -0.05 & 1-0.3 & 0\end{amatrix} \to \begin{amatrix}{2}0.05 & -0.7 & 0\\ 0 & 0 & 0\end{amatrix} \to \begin{amatrix}{2}1 & 0 & 14\\ 0 & 0 & 0\end{amatrix} \implies v_{1} = \begin{bmatrix}14 \\ 1 \end{bmatrix}$

$\begin{amatrix}{2}0.25-0.95 & -0.7 & 0\\ -0.05 & 0.25- 0.3 & 0\end{amatrix} \to \begin{amatrix}{2}-0.7 & -0.7 & 0\\  -0.05& -0.05 & 0\end{amatrix} \to \begin{amatrix}{2}1 & 0 & -1\\ 0 & 0 & 0\end{amatrix} \implies v_{2} = \begin{bmatrix}-1 \\ 1 \end{bmatrix}$\smallskip

\[x_{t} = a_{1} 1^{t}\begin{bmatrix}14 \\ 1\end{bmatrix} + a_{2} (0.25)^{t} \begin{bmatrix}-1 \\ 1\end{bmatrix}\]

c) In the long run, as $t\to\infty$, $(0.25)^{t} \to 0$. $e_{\infty} = 14a_{1}$, $u_{\infty} = a_{1}$. The long run unemployment rate is $\frac{u_{\infty}}{u_{\infty} + e_{\infty}} = \frac{1}{1 + 14} \approx 6.6\%$

\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item We defined $\text{Var}(X) = \E [(X - \E[X])^{2}]$. Show that $\text{Var}(X) = \E[X^{2}] - \E[X]^{2}$.


\ifsln
\textit{Solution:}\\
$\E[(X - \mu)^{2}] = \E[X^{2} - 2\mu X + \mu^{2}] = \E[X^{2}] - 2\mu \E[X] + \mu^{2} = \E[X^{2}] - 2\mu^{2} + \mu^{2} = \E[X^{2}] - \mu^{2} = \E[X^{2}] - \E[X]^{2}$.
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item The pdf of the exponential distribution is 
\[f(x, \lambda) = \begin{cases}\lambda e^{-\lambda x} & \text{if } x\geq 0\\ 0 & \text{if } x < 0 \end{cases}\]
its cdf is
\[F(x, \lambda) = \begin{cases}1 - e^{-\lambda x} & \text{if } x\geq 0\\ 0 & \text{if } x < 0 \end{cases}\]

\begin{enumerate}[a)]
	\item Verify that $f$ is a valid density function.
	\item Verify that $F$ is the distribution function corresponding to this density.
\end{enumerate}

\ifsln
\textit{Solution:}\\
a) $\int_{0}^{\infty} \lambda e^{-\lambda x} \mathrm{d}x = \left[-e^{-\lambda x}\right]_{0}^{1} = \lim_{x\to\infty} -e^{-\lambda x} - - e^{-\lambda \cdot 0} = 0 - -1 = 1 $. Where the limit goes to zero since $\lambda > 0$.\\

b) $F^{\prime}(x; \lambda) = \frac{\mathrm{d}}{\mathrm{d}x} [1 - e^{-\lambda x}] = 0 + \lambda e^{-\lambda x} = f(x; \lambda)$.
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\item Show that $M_{aX + b}(t) = e^{bt}M_{X}(at)$.

\ifsln
\textit{Solution:}\\
$M_{aX+b}(t) = \E[e^{t(aX+b})] = \E[e^{(at)X}e^{bt}] = e^{bt}\E[e^{(at)X}] = e^{bt} M_{X}(at)$.
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item Let $X_{1}, X_{2}, \dots, X_{n} \overset{iid}{\sim} N(\mu, \sigma^{2})$. Show that $\sqrt{n}(\bar{X} - \mu) \sim N(0, \sigma^{2})$.

\ifsln
\textit{Solution:}\\

$M_{X_{i}}(t) = \text{exp}\{\mu t + \sigma^{2}t^{2} / 2\}$\\

$\sqrt{n}(\bar{X} - \mu) = \sqrt{n}(\frac{1}{n}\sum X_{i} - \mu)=\sqrt{n}(\frac{1}{n}\sum (X_{i} - \mu)) = \sum n^{-1/2}(X_{i} - \mu)$.\\

Let $Z_{i} = n^{-1/2}(X_{i} - \mu)$. Note that $Z_{i} = n^{-1/2}X_{i} - n^{-1/2}\mu$ which is of the form $aX_{i} + b$. From the question above, we know how to find the mgf.\\

$M_{Z_{i}}(t) = \text{exp}\{-n^{-1/2}\mu t\} \text{exp}\{ \mu (n^{-1/2}t) + \sigma^{2} (n^{-1/2}t)^{2} / 2\} = \text{exp}\{ - n^{-1/2}\mu t + n^{-1/2}\mu t + n^{-1} \sigma^{2} t^{2} / 2\} = \text{exp}\{  n^{-1} \sigma^{2} t^{2} / 2\}$\\

$M_{\sqrt{n}(\bar{X} - \mu)}(t) = M_{\sum Z_{i}}(t) = \left[ M_{Z_{i}}(t) \right]^{n} =\text{exp}\{  n^{-1} \sigma^{2} t^{2} / 2\}^{n} = \text{exp}\{ \sigma^{2} t^{2} / 2\} $, which is the mgf of a $N(0, \sigma^{2})$.
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\item An economic agent needs to make a decision based on an unknown random variable $\theta \sim N(0, \sigma^{2}_{\theta})$. The agent knows the means and variances of these distributions. They would like to know the realization of $\theta$ exactly but are only given a noisy signal $y = \theta + \epsilon$, where $\epsilon \sim N(0, \sigma_{\epsilon}^{2})$ is iid noise. Suppose the agent's strategy is to guess $\theta$ using a multiple of the signal: $\hat{\theta} = c y$. What constant $c$ should the agent pick in order to minimize expected squared error: $\E [(\hat{\theta} - \theta)^{2}]$?

\ifsln
\textit{Solution:}\\
MSE = $\E[(\hat{\theta} - \theta)^{2}] = \E[ \hat{\theta}^{2} - 2 \hat{\theta}\theta + \theta^{2}] = \E[c^{2}y^{2} - 2cy\theta + \theta^{2}] = c^{2}\E[y^{2}] - 2c\E[y\theta] + \E[\theta^{2}] = c^{2}(\sigma^{2}_{\theta} + \sigma^{2}_{\epsilon}) - 2c \sigma^{2}_{\theta}  + \sigma^{2}\theta$\\

First order condition for a minimum is $\frac{\mathrm{d} MSE}{\mathrm{d}c} = 0 \implies 2c(\sigma^{2}_{\theta} + \sigma^{2}_{\epsilon}) - 2\sigma^{2}_{\theta} = 0 \implies c = \frac{\sigma^{2}_{\theta}}{\sigma^{2}_{\theta} + \sigma^{2}_{\epsilon}}$. \\

Second order condition is $\frac{\mathrm{d}^{2} MSE}{\mathrm{d}c^{2}} = 2 (\sigma^{2}_{\theta} + \sigma^{2}_{\epsilon}) > 0$, so we have a necessary and sufficient condition for a local minimum. Since the objective function is strictly convex, we have a global minimum.
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{enumerate}
\end{document}
